# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ LLM –¥–ª—è benchmark-standalone

> **–î–æ–∫—É–º–µ–Ω—Ç –æ–ø–∏—Å—ã–≤–∞–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞:**
> 1. **–ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ** - —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ WebSocket –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ (2-3 –¥–Ω—è)
> 2. **Event-Driven Architecture** - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Ä–µ—à–µ–Ω–∏–µ (4-6 –Ω–µ–¥–µ–ª—å)

## –ü—Ä–æ–±–ª–µ–º–∞

–°–æ–≥–ª–∞—Å–Ω–æ –∞–Ω–∞–ª–∏–∑—É [`multiagent-analyze/notebook.ipynb`](../multiagent-analyze/notebook.ipynb), benchmark –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **0 LLM –≤—ã–∑–æ–≤–æ–≤, 0 —Ç–æ–∫–µ–Ω–æ–≤, $0.00 —Å—Ç–æ–∏–º–æ—Å—Ç—å**, —Ö–æ—Ç—è —Å–∏—Å—Ç–µ–º–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç 118 tool calls –∏ 43 agent switches.

**–ü—Ä–∏—á–∏–Ω–∞**: `benchmark-standalone` —è–≤–ª—è–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—â–∞–µ—Ç—Å—è —Å —Å–µ—Ä–≤–∏—Å–æ–º —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ REST API –∏ WebSocket. –ú–µ—Ç—Ä–∏–∫–∏ LLM –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ Agent Runtime, –Ω–æ **–Ω–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ** –≤ benchmark-standalone.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
benchmark-standalone (–∫–ª–∏–µ–Ω—Ç)
    ‚Üì WebSocket
Gateway (–ø—Ä–æ–∫—Å–∏)
    ‚Üì HTTP SSE Stream
Agent Runtime (LLM –ª–æ–≥–∏–∫–∞)
    ‚Üì HTTP
LLM Proxy (–≤—ã–∑–æ–≤—ã LLM)
```

## –†–µ—à–µ–Ω–∏–µ: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ WebSocket –ø—Ä–æ—Ç–æ–∫–æ–ª–∞

–î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π —Ç–∏–ø —Å–æ–æ–±—â–µ–Ω–∏—è `llm_metrics` –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –º–µ—Ç—Ä–∏–∫ LLM —á–µ—Ä–µ–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π WebSocket –∫–∞–Ω–∞–ª.

---

## –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ Agent Runtime

### 1. –î–æ–±–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –º–µ—Ç—Ä–∏–∫ LLM

**–§–∞–π–ª**: `codelab-ai-service/agent-runtime/app/models/schemas.py`

```python
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime

class LLMMetrics(BaseModel):
    """–ú–µ—Ç—Ä–∏–∫–∏ LLM –≤—ã–∑–æ–≤–∞ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ benchmark"""
    agent_type: str = Field(..., description="–¢–∏–ø –∞–≥–µ–Ω—Ç–∞ (coder, architect, etc.)")
    model: str = Field(..., description="–ú–æ–¥–µ–ª—å LLM (gpt-4, claude-3, etc.)")
    input_tokens: int = Field(..., ge=0, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
    output_tokens: int = Field(..., ge=0, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
    duration_seconds: float = Field(..., ge=0, description="–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–∑–æ–≤–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    call_id: str = Field(..., description="UUID –≤—ã–∑–æ–≤–∞ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞")

class StreamChunk(BaseModel):
    """Existing model - add llm_metrics field"""
    type: str
    # ... existing fields ...
    
    # NEW: LLM metrics
    llm_metrics: Optional[LLMMetrics] = None
```

### 2. –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å LLM Stream Service –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫

**–§–∞–π–ª**: `codelab-ai-service/agent-runtime/app/services/llm_stream_service.py`

```python
import time
import uuid
from datetime import datetime, timezone

async def stream_response(
    session_id: str,
    history: List[dict],
    allowed_tools: Optional[List[str]] = None,
    session_mgr: Optional[AsyncSessionManager] = None,
    agent_type: str = "unknown"  # NEW: –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä agent_type
) -> AsyncGenerator[StreamChunk, None]:
    """
    Generate streaming response from LLM with metrics collection.
    """
    if session_mgr is None:
        from app.services.session_manager_async import session_manager as global_mgr
        session_mgr = global_mgr
        if session_mgr is None:
            raise RuntimeError("SessionManager not initialized")
    
    try:
        logger.info(
            f"Starting LLM stream for session {session_id} with {len(history)} messages"
        )
        
        # Filter tools based on allowed_tools
        tools_to_use = TOOLS_SPEC
        if allowed_tools is not None:
            tools_to_use = [
                tool for tool in TOOLS_SPEC
                if tool["function"]["name"] in allowed_tools
            ]
        
        # NEW: Start timing
        llm_start_time = time.time()
        call_id = str(uuid.uuid4())
        
        logger.debug(f"LLM call started: call_id={call_id}, agent={agent_type}")
        
        # Call LLM proxy
        response_data = await llm_proxy_client.chat_completion(
            model=AppConfig.LLM_MODEL,
            messages=history,
            tools=tools_to_use,
            stream=False
        )
        
        # NEW: Calculate duration
        llm_duration = time.time() - llm_start_time
        
        # NEW: Extract token usage from response
        usage = response_data.get("usage", {})
        input_tokens = usage.get("prompt_tokens", 0)
        output_tokens = usage.get("completion_tokens", 0)
        
        logger.info(
            f"LLM call completed: call_id={call_id}, "
            f"tokens={input_tokens}/{output_tokens}, "
            f"duration={llm_duration:.2f}s"
        )
        
        # NEW: Create metrics object
        llm_metrics = LLMMetrics(
            agent_type=agent_type,
            model=AppConfig.LLM_MODEL,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            duration_seconds=llm_duration,
            timestamp=datetime.now(timezone.utc),
            call_id=call_id
        )
        
        # Extract message from response
        result_message = response_data["choices"][0]["message"]
        content = result_message.get("content", "")
        metadata = {}
        
        # ... existing tool_calls parsing logic ...
        
        # Handle tool calls
        if tool_calls:
            # ... existing tool call logic ...
            
            # Send tool_call chunk WITH metrics
            chunk = StreamChunk(
                type="tool_call",
                call_id=tool_call.id,
                tool_name=tool_call.tool_name,
                arguments=tool_call.arguments,
                requires_approval=requires_approval,
                llm_metrics=llm_metrics,  # NEW: –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
                is_final=True
            )
            
            logger.debug(f"Yielding tool_call chunk with metrics: {tool_call.tool_name}")
            yield chunk
            return
        
        # Handle regular assistant message
        # ... existing content processing ...
        
        await session_mgr.append_message(session_id, "assistant", clean_content)
        
        # Send assistant message chunk WITH metrics
        chunk = StreamChunk(
            type="assistant_message",
            content=clean_content,
            token=clean_content,
            llm_metrics=llm_metrics,  # NEW: –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            is_final=True
        )
        
        logger.debug("Yielding assistant_message chunk with metrics")
        yield chunk
        
    except Exception as e:
        logger.error(
            f"Exception in stream_response for session {session_id}: {e}",
            exc_info=True
        )
        
        error_chunk = StreamChunk(
            type="error",
            error=str(e),
            is_final=True
        )
        yield error_chunk
```

### 3. –ü–µ—Ä–µ–¥–∞—Ç—å agent_type –≤ stream_response

**–§–∞–π–ª**: `codelab-ai-service/agent-runtime/app/services/multi_agent_orchestrator.py`

```python
async def process_message(
    self,
    session_id: str,
    message: str,
    agent_type: Optional[AgentType] = None
) -> AsyncGenerator[StreamChunk, None]:
    """Process message through multi-agent system"""
    
    # ... existing agent selection logic ...
    
    # Get current agent
    current_agent = agent_router.get_agent(agent_context.current_agent)
    
    logger.info(
        f"Processing with agent: {current_agent.agent_type.value} "
        f"for session {session_id}"
    )
    
    # Stream response from agent WITH agent_type
    async for chunk in stream_response(
        session_id=session_id,
        history=history,
        allowed_tools=current_agent.get_allowed_tools(),
        session_mgr=self.session_mgr,
        agent_type=current_agent.agent_type.value  # NEW: –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–∏–ø –∞–≥–µ–Ω—Ç–∞
    ):
        yield chunk
```

---

## –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ Gateway

### 1. –û–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª–∏ WebSocket

**–§–∞–π–ª**: `codelab-ai-service/gateway/app/models/websocket.py`

```python
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime

class WSLLMMetrics(BaseModel):
    """LLM metrics –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —á–µ—Ä–µ–∑ WebSocket"""
    agent_type: str
    model: str
    input_tokens: int
    output_tokens: int
    duration_seconds: float
    timestamp: datetime
    call_id: str

class WSAssistantMessage(BaseModel):
    """Assistant message —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    type: str = "assistant_message"
    token: str
    is_final: bool = False
    llm_metrics: Optional[WSLLMMetrics] = None  # NEW

class WSToolCall(BaseModel):
    """Tool call —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    type: str = "tool_call"
    call_id: str
    tool_name: str
    arguments: dict
    requires_approval: bool = False
    llm_metrics: Optional[WSLLMMetrics] = None  # NEW
```

### 2. –ü—Ä–æ–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —á–µ—Ä–µ–∑ Gateway WebSocket

**–§–∞–π–ª**: `codelab-ai-service/gateway/app/api/v1/endpoints.py`

–í —Ñ—É–Ω–∫—Ü–∏–∏ `websocket_endpoint`, –≤ –±–ª–æ–∫–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ SSE stream:

```python
# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏
if line.startswith("data: "):
    data_str = line[6:]
    
    if data_str == "[DONE]":
        logger.info(f"[{session_id}] Received [DONE] marker")
        break
    
    if current_event_type == "message":
        try:
            data = json.loads(data_str)
            msg_type = data.get('type')
            
            # NEW: –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Ç—Ä–∏–∫–∏ LLM
            if 'llm_metrics' in data and data['llm_metrics']:
                metrics = data['llm_metrics']
                logger.info(
                    f"[{session_id}] LLM metrics: agent={metrics.get('agent_type')}, "
                    f"tokens={metrics.get('input_tokens')}/{metrics.get('output_tokens')}, "
                    f"model={metrics.get('model')}"
                )
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º null –∑–Ω–∞—á–µ–Ω–∏—è
            filtered_data = {k: v for k, v in data.items() if v is not None}
            
            logger.debug(f"[{session_id}] Forwarding to IDE: type={msg_type}")
            
            # –ü–µ—Ä–µ—Å—ã–ª–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –≤ IDE —á–µ—Ä–µ–∑ WebSocket (–≤–∫–ª—é—á–∞—è llm_metrics)
            await websocket.send_json(filtered_data)
            
        except json.JSONDecodeError as e:
            logger.warning(f"[{session_id}] Failed to parse SSE data: {e}")
```

---

## –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ benchmark-standalone

### 1. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å llm_metrics –≤ WebSocket –∫–ª–∏–µ–Ω—Ç–µ

**–§–∞–π–ª**: `benchmark-standalone/src/client.py`

–í –º–µ—Ç–æ–¥–µ `execute_task`, –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –º–µ—Ç—Ä–∏–∫:

```python
async def execute_task(
    self,
    task: Dict[str, Any],
    tool_executor: MockToolExecutor,
    validator: Optional[TaskValidator],
    collector: MetricsCollector,
    task_execution_id: UUID
) -> bool:
    """Execute task via Gateway WebSocket with metrics collection"""
    
    # ... existing setup ...
    
    try:
        ws_endpoint = f"{self.ws_url}/{session_id}"
        async with websockets.connect(ws_endpoint) as websocket:
            logger.info(f"üîå Connected to Gateway WebSocket")
            
            # Send initial message
            await websocket.send(json.dumps({
                "type": "user_message",
                "content": task_description,
                "role": "user"
            }))
            
            # Process responses
            while True:
                try:
                    data = await asyncio.wait_for(
                        websocket.recv(),
                        timeout=self.timeout
                    )
                    msg = json.loads(data)
                    msg_type = msg.get("type")
                    
                    # NEW: Extract and record LLM metrics if present
                    llm_metrics = msg.get("llm_metrics")
                    if llm_metrics:
                        logger.info(
                            f"üìä LLM metrics: agent={llm_metrics.get('agent_type')}, "
                            f"tokens={llm_metrics.get('input_tokens')}/"
                            f"{llm_metrics.get('output_tokens')}, "
                            f"model={llm_metrics.get('model')}, "
                            f"duration={llm_metrics.get('duration_seconds'):.2f}s"
                        )
                        
                        # Record LLM call metric
                        await collector.record_llm_call(
                            task_execution_id=task_execution_id,
                            agent_type=llm_metrics.get('agent_type', 'unknown'),
                            input_tokens=llm_metrics.get('input_tokens', 0),
                            output_tokens=llm_metrics.get('output_tokens', 0),
                            model=llm_metrics.get('model', 'unknown'),
                            duration_seconds=llm_metrics.get('duration_seconds', 0.0)
                        )
                    
                    if msg_type == "assistant_message":
                        token = msg.get("token", "")
                        response_text += token
                        
                        if msg.get("is_final"):
                            logger.info(f"‚úÖ Received final message ({len(response_text)} chars)")
                            break
                    
                    elif msg_type == "tool_call":
                        tool_calls_count += 1
                        
                        call_id = msg.get("call_id")
                        tool_name = msg.get("tool_name")
                        arguments = msg.get("arguments", {})
                        
                        logger.info(
                            f"üîß Tool call #{tool_calls_count}: {tool_name} "
                            f"(call_id={call_id[:8]}...)"
                        )
                        
                        # Execute tool locally
                        start_time = time.time()
                        tool_result = await tool_executor.execute_tool(
                            tool_name, arguments
                        )
                        duration = time.time() - start_time
                        
                        # Record tool call metric
                        await collector.record_tool_call(
                            task_execution_id=task_execution_id,
                            tool_name=tool_name,
                            success=tool_result.get('success', False),
                            duration_seconds=duration,
                            error=tool_result.get('error')
                        )
                        
                        # Send tool result back
                        await websocket.send(json.dumps({
                            "type": "tool_result",
                            "call_id": call_id,
                            "result": tool_result
                        }))
                    
                    # ... existing agent_switched, error handling ...
                    
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout waiting for response ({self.timeout}s)")
                    has_error = True
                    break
            
            # ... existing validation and completion logic ...
```

### 2. –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –º–µ—Ç—Ä–∏–∫

**–§–∞–π–ª**: `benchmark-standalone/src/collector.py`

```python
async def get_experiment_summary(self, experiment_id: UUID) -> Dict[str, Any]:
    """Get summary with LLM metrics validation"""
    
    # ... existing code ...
    
    # Calculate total tokens
    total_input_tokens = 0
    total_output_tokens = 0
    total_llm_calls = 0
    
    for task in tasks:
        total_input_tokens += sum(call.input_tokens for call in task.llm_calls)
        total_output_tokens += sum(call.output_tokens for call in task.llm_calls)
        total_llm_calls += len(task.llm_calls)
    
    # NEW: Validate benchmark
    if total_llm_calls == 0:
        logger.warning(
            f"‚ö†Ô∏è BENCHMARK INVALID: No LLM calls detected for experiment {experiment_id}. "
            f"Multi-Agent system must make at least 1 LLM call per task."
        )
    
    # Calculate cost
    cost = (total_input_tokens * 0.003 + total_output_tokens * 0.015) / 1000
    
    return {
        "experiment_id": str(experiment_id),
        "mode": experiment.mode,
        "total_tasks": total_tasks,
        "successful_tasks": successful_tasks,
        "failed_tasks": failed_tasks,
        "success_rate": successful_tasks / total_tasks if total_tasks > 0 else 0.0,
        "total_input_tokens": total_input_tokens,
        "total_output_tokens": total_output_tokens,
        "total_llm_calls": total_llm_calls,  # NEW
        "estimated_cost_usd": round(cost, 4),
        "is_valid": total_llm_calls > 0  # NEW: —Ñ–ª–∞–≥ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    }
```

---

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫

### –§–æ—Ä–º–∞—Ç –ø–µ—Ä–µ–¥–∞—á–∏ —á–µ—Ä–µ–∑ WebSocket

```json
{
  "type": "assistant_message",
  "token": "Here is the solution...",
  "is_final": true,
  "llm_metrics": {
    "agent_type": "coder",
    "model": "gpt-4",
    "input_tokens": 1250,
    "output_tokens": 450,
    "duration_seconds": 3.45,
    "timestamp": "2026-01-17T12:00:00.000Z",
    "call_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
  }
}
```

### –•—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö

–¢–∞–±–ª–∏—Ü–∞ `poc_llm_calls` —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ [`benchmark-standalone/src/models.py`](../benchmark-standalone/src/models.py:195):

```python
class LLMCall(Base):
    """LLM API call tracking."""
    __tablename__ = "poc_llm_calls"
    
    id: Mapped[str] = mapped_column(String(36), primary_key=True)
    task_execution_id: Mapped[str] = mapped_column(String(36), ForeignKey(...))
    agent_type: Mapped[str] = mapped_column(String(50), nullable=False)
    input_tokens: Mapped[int] = mapped_column(Integer, nullable=False)
    output_tokens: Mapped[int] = mapped_column(Integer, nullable=False)
    model: Mapped[str] = mapped_column(String(100), nullable=False)
    duration_seconds: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    # ... timestamps ...
```

---

## –ü–ª–∞–Ω –≤–Ω–µ–¥—Ä–µ–Ω–∏—è

### –§–∞–∑–∞ 1: Agent Runtime (Priority 0)
1. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å `LLMMetrics` –º–æ–¥–µ–ª—å –≤ `app/models/schemas.py`
2. ‚úÖ –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å `llm_stream_service.py` –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
3. ‚úÖ –ü–µ—Ä–µ–¥–∞—Ç—å `agent_type` –≤ `stream_response` –∏–∑ orchestrator
4. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ LLM

### –§–∞–∑–∞ 2: Gateway
1. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å WebSocket –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ `llm_metrics`
2. ‚úÖ –ü—Ä–æ–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —á–µ—Ä–µ–∑ Gateway –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
3. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ Gateway

### –§–∞–∑–∞ 3: benchmark-standalone
1. ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å `llm_metrics` –≤ WebSocket –∫–ª–∏–µ–Ω—Ç–µ
2. ‚úÖ –ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —á–µ—Ä–µ–∑ `MetricsCollector.record_llm_call()`
3. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é: `total_llm_calls > 0`
4. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å –æ—Ç—á–µ—Ç—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ LLM

### –§–∞–∑–∞ 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
1. ‚úÖ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–¥–Ω—É –∑–∞–¥–∞—á—É –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
2. ‚úÖ –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ `total_llm_calls > 0`
3. ‚úÖ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
4. ‚úÖ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π benchmark

---

## –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è

### –í–∞—Ä–∏–∞–Ω—Ç 2: REST API endpoint –¥–ª—è –º–µ—Ç—Ä–∏–∫

–î–æ–±–∞–≤–∏—Ç—å endpoint –≤ Agent Runtime:

```python
@router.get("/sessions/{session_id}/metrics")
async def get_session_metrics(session_id: str):
    """Get LLM metrics for session"""
    # Query from database or in-memory cache
    return {
        "llm_calls": [...],
        "total_input_tokens": 1250,
        "total_output_tokens": 450,
        "total_cost": 0.0105
    }
```

**–ú–∏–Ω—É—Å—ã**:
- –¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
- –°–ª–æ–∂–Ω–µ–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å WebSocket –ø–æ—Ç–æ–∫–æ–º
- –ù—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ –ø–∞–º—è—Ç–∏ –∏–ª–∏ –ë–î

### –í–∞—Ä–∏–∞–Ω—Ç 3: Server-Sent Events (SSE) –¥–ª—è –º–µ—Ç—Ä–∏–∫

–û—Ç–¥–µ–ª—å–Ω—ã–π SSE stream –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å WebSocket.

**–ú–∏–Ω—É—Å—ã**:
- –£—Å–ª–æ–∂–Ω—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
- –î–≤–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ
- –°–ª–æ–∂–Ω–µ–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—ã—Ç–∏—è

---

## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è

1. ‚úÖ **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è** - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π WebSocket –∫–∞–Ω–∞–ª
2. ‚úÖ **–†–µ–∞–ª-—Ç–∞–π–º** - –º–µ—Ç—Ä–∏–∫–∏ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ LLM –≤—ã–∑–æ–≤–∞
3. ‚úÖ **–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** - `llm_metrics` –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø–æ–ª–µ
4. ‚úÖ **–ü—Ä–æ—Å—Ç–æ—Ç–∞** - –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –Ω–æ–≤—ã—Ö endpoints –∏–ª–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
5. ‚úÖ **–¢—Ä–µ–π—Å–∏–Ω–≥** - `call_id` –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–≤—è–∑–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Å tool calls

---

## –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

–ü–æ—Å–ª–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è, benchmark –¥–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å:

```
Total LLM Calls: 45
Input Tokens: 18,750
Output Tokens: 6,200
Total Tokens: 24,950
Estimated Cost: $0.15
```

–í–º–µ—Å—Ç–æ —Ç–µ–∫—É—â–∏—Ö:

```
Total LLM Calls: 0  ‚ùå
Input Tokens: 0     ‚ùå
Output Tokens: 0    ‚ùå
Total Tokens: 0     ‚ùå
Estimated Cost: $0.00  ‚ùå
```

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

### 1. –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∞–≥–µ–Ω—Ç–∞–º

```python
# –í –æ—Ç—á–µ—Ç–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å breakdown –ø–æ –∞–≥–µ–Ω—Ç–∞–º
{
    "coder": {"calls": 25, "tokens": 15000},
    "architect": {"calls": 10, "tokens": 5000},
    "debug": {"calls": 10, "tokens": 4950}
}
```

### 2. Latency –º–µ—Ç—Ä–∏–∫–∏

```python
# –î–æ–±–∞–≤–∏—Ç—å –≤ LLMMetrics
class LLMMetrics(BaseModel):
    # ... existing fields ...
    ttft: Optional[float] = None  # Time To First Token
    tps: Optional[float] = None   # Tokens Per Second
```

### 3. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫

```python
# –í Agent Runtime –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Å–µ—Å—Å–∏–∏
class SessionMetricsCache:
    def __init__(self):
        self._cache: Dict[str, List[LLMMetrics]] = {}
    
    def add(self, session_id: str, metrics: LLMMetrics):
        if session_id not in self._cache:
            self._cache[session_id] = []
        self._cache[session_id].append(metrics)
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–î–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç benchmark-standalone —Å–æ–±–∏—Ä–∞—Ç—å –ø–æ–ª–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ LLM, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–µ—Ä–≤–∏—Å–æ–º –∏ REST/WebSocket –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π. –ò–∑–º–µ–Ω–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã, –æ–±—Ä–∞—Ç–Ω–æ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã –∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ WebSocket.

–ü–æ—Å–ª–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è, –±–µ–Ω—á–º–∞—Ä–∫ —Å—Ç–∞–Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–º –∏ –ø–æ–∑–≤–æ–ª–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å Single-Agent –∏ Multi-Agent –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º:
- Task Success Rate
- Time To Useful Answer
- Cost per Task
- LLM Calls per Task
- Token Efficiency
