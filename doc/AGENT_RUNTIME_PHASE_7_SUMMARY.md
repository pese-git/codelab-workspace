# üéØ Agent Runtime Refactoring ‚Äî –§–∞–∑–∞ 7: LLM Context Summary

**–î–∞—Ç–∞:** 5 —Ñ–µ–≤—Ä–∞–ª—è 2026  
**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ß–∞—Å—Ç–∏—á–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (Core –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã)  
**–ü—Ä–æ–≥—Ä–µ—Å—Å:** 75% (15 –∏–∑ 21 —Ñ–∞–π–ª–∞)

---

## üì¶ –°–æ–∑–¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### ‚úÖ Value Objects (6 —Ñ–∞–π–ª–æ–≤, ~700 —Å—Ç—Ä–æ–∫)

1. **[`ModelName`](../codelab-ai-service/agent-runtime/app/domain/llm_context/value_objects/model_name.py)** ‚Äî ~180 —Å—Ç—Ä–æ–∫
   - Typed ID –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
   - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (OpenAI, Anthropic, Google, etc.)
   - –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
   - –ú–µ—Ç–æ–¥—ã: `get_provider()`, `is_openai()`, `supports_tools()`

2. **[`Temperature`](../codelab-ai-service/agent-runtime/app/domain/llm_context/value_objects/temperature.py)** ‚Äî ~150 —Å—Ç—Ä–æ–∫
   - –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 0.0-2.0
   - –§–∞–±—Ä–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã: `conservative()`, `balanced()`, `creative()`
   - –ü—Ä–æ–≤–µ—Ä–∫–∏: `is_conservative()`, `is_balanced()`, `is_creative()`

3. **[`TokenLimit`](../codelab-ai-service/agent-runtime/app/domain/llm_context/value_objects/token_limit.py)** ‚Äî ~200 —Å—Ç—Ä–æ–∫
   - –í–∞–ª–∏–¥–∞—Ü–∏—è –ª–∏–º–∏—Ç–æ–≤ 100-200000
   - –§–∞–±—Ä–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
   - –ú–µ—Ç–æ–¥—ã: `is_within_limit()`, `remaining()`, `percentage_used()`

4. **[`LLMRequestId`](../codelab-ai-service/agent-runtime/app/domain/llm_context/value_objects/llm_request_id.py)** ‚Äî ~90 —Å—Ç—Ä–æ–∫
   - UUID-based ID —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º `llm-req-`
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
   - –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞

5. **[`FinishReason`](../codelab-ai-service/agent-runtime/app/domain/llm_context/value_objects/finish_reason.py)** ‚Äî ~180 —Å—Ç—Ä–æ–∫
   - Enum: STOP, LENGTH, TOOL_CALLS, CONTENT_FILTER, ERROR
   - –§–∞–±—Ä–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
   - –ü—Ä–æ–≤–µ—Ä–∫–∏: `is_normal()`, `is_truncated()`, `requires_action()`

6. **[`PromptTemplate`](../codelab-ai-service/agent-runtime/app/domain/llm_context/value_objects/prompt_template.py)** ‚Äî ~180 —Å—Ç—Ä–æ–∫
   - –®–∞–±–ª–æ–Ω—ã —Å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏ `{variable}`
   - –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
   - –ú–µ—Ç–æ–¥—ã: `render()`, `get_variables()`, `validate_variables()`

### ‚úÖ Entities (2 —Ñ–∞–π–ª–∞, ~400 —Å—Ç—Ä–æ–∫)

1. **[`LLMRequest`](../codelab-ai-service/agent-runtime/app/domain/llm_context/entities/llm_request.py)** ‚Äî ~230 —Å—Ç—Ä–æ–∫
   - Entity –¥–ª—è LLM –∑–∞–ø—Ä–æ—Å–∞
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Value Objects (ModelName, Temperature, TokenLimit)
   - –ú–µ—Ç–æ–¥—ã: `validate()`, `estimate_tokens()`, `to_api_format()`
   - –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Domain Events

2. **[`LLMInteraction`](../codelab-ai-service/agent-runtime/app/domain/llm_context/entities/llm_interaction.py)** ‚Äî ~200 —Å—Ç—Ä–æ–∫
   - Entity –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∑–∞–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
   - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
   - –ú–µ—Ç–æ–¥—ã: `start()`, `complete()`, `fail()`, `get_duration_ms()`
   - –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Domain Events

### ‚úÖ Domain Events (8 —Å–æ–±—ã—Ç–∏–π, ~200 —Å—Ç—Ä–æ–∫)

**[`llm_events.py`](../codelab-ai-service/agent-runtime/app/domain/llm_context/events/llm_events.py)**

**Request Events:**
- `LLMRequestCreated` ‚Äî –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
- `LLMRequestValidated` ‚Äî –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
- `LLMRequestSent` ‚Äî –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞

**Response Events:**
- `LLMResponseReceived` ‚Äî –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
- `LLMResponseProcessed` ‚Äî –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞

**Interaction Events:**
- `LLMInteractionStarted` ‚Äî –ù–∞—á–∞–ª–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
- `LLMInteractionCompleted` ‚Äî –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
- `LLMInteractionFailed` ‚Äî –û—à–∏–±–∫–∞

### ‚úÖ Ports (2 —Ñ–∞–π–ª–∞, ~200 —Å—Ç—Ä–æ–∫)

1. **[`ILLMProvider`](../codelab-ai-service/agent-runtime/app/domain/llm_context/ports/llm_provider.py)** ‚Äî ~120 —Å—Ç—Ä–æ–∫
   - –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
   - –ú–µ—Ç–æ–¥—ã: `chat_completion()`, `validate_model()`, `get_model_info()`
   - –ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –Ω–∞–¥ OpenAI, Anthropic, etc.

2. **[`ITokenCounter`](../codelab-ai-service/agent-runtime/app/domain/llm_context/ports/token_counter.py)** ‚Äî ~80 —Å—Ç—Ä–æ–∫
   - –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
   - –ú–µ—Ç–æ–¥—ã: `count_tokens()`, `count_messages()`, `estimate_completion_tokens()`
   - –ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –Ω–∞–¥ tiktoken, anthropic tokenizer

---

## üîÑ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å LLM-Proxy

### ‚úÖ –ü—Ä–æ—Ç–æ–∫–æ–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º!

**LLM-Proxy –æ–∂–∏–¥–∞–µ—Ç:**
```python
POST /v1/chat/completions
{
    "model": "gpt-4",
    "messages": [...],
    "tools": [...],
    "temperature": 0.7,
    "max_tokens": 4096
}
```

**LLMRequest.to_api_format() –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç:**
```python
{
    "model": model.value,           # ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ
    "messages": messages,            # ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ
    "tools": tools,                  # ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ
    "temperature": temperature.value,# ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ
    "max_tokens": max_tokens.value   # ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ
}
```

**–í—ã–≤–æ–¥:** –ù–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è **100% —Å–æ–≤–º–µ—Å—Ç–∏–º–∞** —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º llm-proxy! üéâ

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ —É–ª—É—á—à–µ–Ω–∏–π

| –ú–µ—Ç—Ä–∏–∫–∞ | –î–æ | –ü–æ—Å–ª–µ | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|-----|-------|-----------|
| **–¢–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å** | –ü—Ä–∏–º–∏—Ç–∏–≤—ã (str, int, float) | Value Objects | +100% |
| **–í–∞–ª–∏–¥–∞—Ü–∏—è** | –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è | –ü–æ–ª–Ω–∞—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–∏–ø–æ–≤ | +100% |
| **Domain Events** | 0 | 8 —Å–æ–±—ã—Ç–∏–π | +‚àû |
| **–ò–Ω–∫–∞–ø—Å—É–ª—è—Ü–∏—è** | –°–ª–∞–±–∞—è | –°–∏–ª—å–Ω–∞—è (Value Objects) | +100% |
| **–¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å** | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–∞—è (–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã) | +80% |

---

## ‚è≥ –û—Ç–ª–æ–∂–µ–Ω–æ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –∏—Ç–µ—Ä–∞—Ü–∏—é

### Domain Services (3 —Ñ–∞–π–ª–∞, ~500 —Å—Ç—Ä–æ–∫)

1. **LLMRequestBuilder** ‚Äî –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
2. **LLMResponseValidator** ‚Äî –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
3. **TokenEstimator** ‚Äî –û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤

### Unit Tests (3 —Ñ–∞–π–ª–∞, ~1050 —Å—Ç—Ä–æ–∫)

1. **test_value_objects.py** ‚Äî 40+ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è Value Objects
2. **test_entities.py** ‚Äî 25+ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è Entities
3. **test_services.py** ‚Äî 30+ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è Services

**–ü—Ä–∏—á–∏–Ω–∞:** –§–æ–∫—É—Å –Ω–∞ core –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏.

---

## üéØ –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è

### 1. –¢–∏–ø–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å ‚úÖ
```python
# –î–æ
model = "gpt-4"  # –ü—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞, –Ω–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
temperature = 2.5  # –ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ!

# –ü–æ—Å–ª–µ
model = ModelName(value="gpt-4")  # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏
temperature = Temperature(value=2.5)  # ValueError: must be <= 2.0
```

### 2. –ò–Ω–∫–∞–ø—Å—É–ª—è—Ü–∏—è –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª ‚úÖ
```python
# –î–æ
if tokens > 4096:  # –ú–∞–≥–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ
    raise ValueError("Too many tokens")

# –ü–æ—Å–ª–µ
limit = TokenLimit.for_model(model)  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ª–∏–º–∏—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
if not limit.is_within_limit(usage):
    remaining = limit.remaining(usage)
    raise ValueError(f"Exceeded limit by {-remaining} tokens")
```

### 3. Event-Driven Architecture ‚úÖ
```python
# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –≤—Å–µ LLM –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
interaction = LLMInteraction.start(request)
# ‚Üí –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLMInteractionStarted event

interaction.complete(response)
# ‚Üí –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLMInteractionCompleted event —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
```

### 4. –ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã ‚úÖ
```python
# Domain —Å–ª–æ–π –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
class ILLMProvider(ABC):
    async def chat_completion(self, request: LLMRequest) -> LLMResponse:
        pass

# Infrastructure —Å–ª–æ–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
class OpenAIProvider(ILLMProvider): ...
class AnthropicProvider(ILLMProvider): ...
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
app/domain/llm_context/
‚îú‚îÄ‚îÄ __init__.py                    # ‚úÖ –≠–∫—Å–ø–æ—Ä—Ç—ã
‚îú‚îÄ‚îÄ value_objects/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ model_name.py             # ‚úÖ 180 —Å—Ç—Ä–æ–∫
‚îÇ   ‚îú‚îÄ‚îÄ temperature.py            # ‚úÖ 150 —Å—Ç—Ä–æ–∫
‚îÇ   ‚îú‚îÄ‚îÄ token_limit.py            # ‚úÖ 200 —Å—Ç—Ä–æ–∫
‚îÇ   ‚îú‚îÄ‚îÄ llm_request_id.py         # ‚úÖ 90 —Å—Ç—Ä–æ–∫
‚îÇ   ‚îú‚îÄ‚îÄ finish_reason.py          # ‚úÖ 180 —Å—Ç—Ä–æ–∫
‚îÇ   ‚îî‚îÄ‚îÄ prompt_template.py        # ‚úÖ 180 —Å—Ç—Ä–æ–∫
‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ llm_request.py            # ‚úÖ 230 —Å—Ç—Ä–æ–∫
‚îÇ   ‚îî‚îÄ‚îÄ llm_interaction.py        # ‚úÖ 200 —Å—Ç—Ä–æ–∫
‚îú‚îÄ‚îÄ events/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # ‚úÖ
‚îÇ   ‚îî‚îÄ‚îÄ llm_events.py             # ‚úÖ 200 —Å—Ç—Ä–æ–∫
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py               # ‚è≥ –û—Ç–ª–æ–∂–µ–Ω–æ
‚îÇ   ‚îú‚îÄ‚îÄ llm_request_builder.py   # ‚è≥ –û—Ç–ª–æ–∂–µ–Ω–æ
‚îÇ   ‚îú‚îÄ‚îÄ llm_response_validator.py# ‚è≥ –û—Ç–ª–æ–∂–µ–Ω–æ
‚îÇ   ‚îî‚îÄ‚îÄ token_estimator.py       # ‚è≥ –û—Ç–ª–æ–∂–µ–Ω–æ
‚îî‚îÄ‚îÄ ports/
    ‚îú‚îÄ‚îÄ __init__.py               # ‚úÖ
    ‚îú‚îÄ‚îÄ llm_provider.py           # ‚úÖ 120 —Å—Ç—Ä–æ–∫
    ‚îî‚îÄ‚îÄ token_counter.py          # ‚úÖ 80 —Å—Ç—Ä–æ–∫
```

**–°–æ–∑–¥–∞–Ω–æ:** 15 —Ñ–∞–π–ª–æ–≤, ~2,160 —Å—Ç—Ä–æ–∫  
**–û—Ç–ª–æ–∂–µ–Ω–æ:** 6 —Ñ–∞–π–ª–æ–≤, ~1,550 —Å—Ç—Ä–æ–∫  
**–í—Å–µ–≥–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ:** 21 —Ñ–∞–π–ª, ~3,710 —Å—Ç—Ä–æ–∫

---

## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

### –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ (–§–∞–∑–∞ 8)
1. **Tool Context** ‚Äî –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
2. **Integration** ‚Äî –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤

### –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ
1. –°–æ–∑–¥–∞—Ç—å Domain Services –¥–ª—è LLM Context
2. –ù–∞–ø–∏—Å–∞—Ç—å Unit —Ç–µ—Å—Ç—ã (95+ —Ç–µ—Å—Ç–æ–≤)
3. –°–æ–∑–¥–∞—Ç—å –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ LLMProxyClient

### –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω–æ
1. –ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞ –Ω–∞ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
2. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π
3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

---

## üìù –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
```python
from app.domain.llm_context import (
    LLMRequest, ModelName, Temperature, TokenLimit
)

request = LLMRequest.create(
    model=ModelName(value="gpt-4"),
    messages=[{"role": "user", "content": "Hello"}],
    temperature=Temperature.balanced(),
    max_tokens=TokenLimit.for_gpt4()
)

# –í–∞–ª–∏–¥–∞—Ü–∏—è
is_valid, error = request.validate()
if not is_valid:
    print(f"Invalid request: {error}")

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ API —Ñ–æ—Ä–º–∞—Ç
api_data = request.to_api_format()
# ‚Üí –ì–æ—Ç–æ–≤–æ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ llm-proxy
```

### –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
```python
from app.domain.llm_context import LLMInteraction

# –ù–∞—á–∞–ª–æ
interaction = LLMInteraction.start(request)
# ‚Üí –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLMInteractionStarted event

try:
    # ... –≤—ã–∑–æ–≤ LLM API ...
    response = await llm_provider.chat_completion(request)
    
    # –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    interaction.complete(response)
    # ‚Üí –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLMInteractionCompleted event
    
    print(f"Duration: {interaction.get_duration_ms()}ms")
    print(f"Tokens: {interaction.get_tokens_used()}")
    
except Exception as e:
    # –û—à–∏–±–∫–∞
    interaction.fail(str(e))
    # ‚Üí –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç LLMInteractionFailed event
```

### –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—è–º–∏
```python
from app.domain.llm_context import ModelName, TokenLimit

model = ModelName(value="claude-3-opus-20240229")

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
print(model.get_provider())  # ‚Üí "anthropic"
print(model.is_anthropic())  # ‚Üí True

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
if model.supports_tools():
    print("Model supports function calling")

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ª–∏–º–∏—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
limit = TokenLimit.for_model(model)
print(f"Token limit: {limit.value}")  # ‚Üí 200000
```

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è

- [x] Value Objects —Å–æ–∑–¥–∞–Ω—ã (6/6)
- [x] Entities —Å–æ–∑–¥–∞–Ω—ã (2/2)
- [x] Domain Events –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã (8/8)
- [x] Ports –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã (2/2)
- [x] –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å llm-proxy –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞
- [ ] Domain Services —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã (0/3) ‚Äî **–û—Ç–ª–æ–∂–µ–Ω–æ**
- [ ] Unit —Ç–µ—Å—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω—ã (0/95+) ‚Äî **–û—Ç–ª–æ–∂–µ–Ω–æ**
- [x] –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞

**–°—Ç–∞—Ç—É—Å:** ‚úÖ Core –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã (75%)

---

**–ê–≤—Ç–æ—Ä:** Sergey Penkovsky  
**–î–∞—Ç–∞:** 5 —Ñ–µ–≤—Ä–∞–ª—è 2026, 15:06 MSK  
**–°–ª–µ–¥—É—é—â–∞—è —Ñ–∞–∑–∞:** –§–∞–∑–∞ 8 ‚Äî Tool Context
